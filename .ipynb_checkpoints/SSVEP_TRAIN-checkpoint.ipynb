{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Flatten\n",
    "import keras.backend as K\n",
    "from keras.constraints import max_norm\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers.core import Dense, Activation, Dropout, Permute, Reshape\n",
    "from keras.callbacks import Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "def square(x):\n",
    "    return K.square(x)\n",
    "\n",
    "def log(x):\n",
    "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000))   \n",
    "\n",
    "def safe_log(x):\n",
    "    return K.log(x + 1e-7)\n",
    "\n",
    "def process_data(train, label, n_class, day_index=1, subject_index=1, shuffle=True):\n",
    "    train = np.squeeze(train[:,:,np.where(label[:,1]==day_index)])\n",
    "    train = np.transpose(train, axes=[2,0,1])\n",
    "    train = train.reshape(train.shape[0], 1, train.shape[1], train.shape[2])\n",
    "    \n",
    "    label = np.squeeze(label[np.where(label[:,1]==day_index),0])   \n",
    "    if n_class == 2:\n",
    "        label[np.where(label != subject_index)] = subject_index + 1\n",
    "        label = keras.utils.to_categorical(label-subject_index, n_class)\n",
    "    else:\n",
    "        label = keras.utils.to_categorical(label-1, n_class)\n",
    "        \n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(train.shape[0])\n",
    "        train = train[permutation,:,:]\n",
    "        label = label[permutation,:]\n",
    "        \n",
    "    return train, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = '/Users/dustintt123/SSVEP_CNN/SSVEP_Data/'\n",
    "SSVEP_l = io.loadmat(datapath+'SSVEP_l.mat')['SSVEP_l']\n",
    "SSVEP_h = io.loadmat(datapath+'SSVEP_h.mat')['SSVEP_h']\n",
    "SSVEP_label = io.loadmat(datapath+'SSVEP_label.mat')['label'] # i_subj, i_session, i_stim, i_trial\n",
    "\n",
    "# binary one-vs-all classification\n",
    "n_class = 2\n",
    "subject = 8\n",
    "train_day = 1\n",
    "\n",
    "x_train, y_train = process_data(SSVEP_l, SSVEP_label, n_class=n_class, day_index=train_day, subject_index=subject)\n",
    "x_test, y_test = process_data(SSVEP_l, SSVEP_label, n_class=n_class, day_index=train_day+1, subject_index=subject)\n",
    "\n",
    "n_trial, n_height, n_channel, n_timestamp = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Keras implementation of the Shallow Convolutional Network as described\n",
    "    in Schirrmeister et. al. (2017), arXiv 1703.0505\n",
    "    \n",
    "    Assumes the input is a 2-second EEG signal sampled at 128Hz. Note that in \n",
    "    the original paper, they do temporal convolutions of length 25 for EEG\n",
    "    data sampled at 250Hz. We instead use length 13 since the sampling rate is \n",
    "    roughly half of the 250Hz which the paper used. The pool_size and stride\n",
    "    in later layers is also approximately half of what is used in the paper.\n",
    "    \n",
    "                     ours        original paper\n",
    "    pool_size        1, 35       1, 75\n",
    "    strides          1, 7        1, 15\n",
    "    conv filters     1, 13       1, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-305-e9740e09603d>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-305-e9740e09603d>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    if conv1_size = (1, 1):\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ShallowConvNet(input_shape, conv1_size=(1, 25), conv2_size=(9, 1), conv1_filter=10, conv2_filter=10):\n",
    "    dropout_rate = 0.5\n",
    "    \n",
    "    # start the model\n",
    "    input_EEG = Input(input_shape)\n",
    "    if conv1_size = (1, 1):\n",
    "        block1       = Conv2D(conv2_filter, conv2_size, use_bias=True, kernel_constraint = max_norm(2.))(input_EEG)\n",
    "    else:\n",
    "        block1       = Conv2D(conv1_filter, conv1_size, kernel_constraint = max_norm(2.))(input_EEG)\n",
    "        block1       = Conv2D(conv2_filter, conv2_size, use_bias=True, kernel_constraint = max_norm(2.))(block1)\n",
    "    block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\n",
    "    block1       = Activation(square)(block1)\n",
    "    block1       = AveragePooling2D(pool_size=(1, 30), strides=(1, 10))(block1)\n",
    "    block1       = Activation(safe_log)(block1)\n",
    "    block1       = Dropout(dropout_rate)(block1)\n",
    "    flatten      = Flatten()(block1)\n",
    "    dense        = Dense(n_class, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input_EEG, outputs=softmax)\n",
    "\n",
    "class AccLossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.loss = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append([logs.get('acc'), logs.get('val_acc')])\n",
    "        self.loss.append([logs.get('loss'), logs.get('val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1440 samples, validate on 480 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 13s 9ms/step - loss: 0.6647 - acc: 0.6986 - val_loss: 0.4296 - val_acc: 0.8146\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.4390 - acc: 0.8160 - val_loss: 0.3465 - val_acc: 0.8437\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.3516 - acc: 0.8583 - val_loss: 0.3390 - val_acc: 0.8562\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3368 - acc: 0.8583 - val_loss: 0.2855 - val_acc: 0.8708\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.2708 - acc: 0.8924 - val_loss: 0.3353 - val_acc: 0.8500\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.2793 - acc: 0.8944 - val_loss: 0.2548 - val_acc: 0.8875\n",
      "Epoch 7/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.2365 - acc: 0.8979 - val_loss: 0.3347 - val_acc: 0.8437\n",
      "Epoch 8/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.2716 - acc: 0.8875 - val_loss: 0.2388 - val_acc: 0.8938\n",
      "Epoch 9/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2304 - acc: 0.9139 - val_loss: 0.1850 - val_acc: 0.9292\n",
      "Epoch 10/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1916 - acc: 0.9250 - val_loss: 0.1875 - val_acc: 0.9208\n",
      "Epoch 11/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1884 - acc: 0.9201 - val_loss: 0.1957 - val_acc: 0.9313\n",
      "Epoch 12/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1781 - acc: 0.9271 - val_loss: 0.1635 - val_acc: 0.9375\n",
      "Epoch 13/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1782 - acc: 0.9306 - val_loss: 0.1818 - val_acc: 0.9250\n",
      "Epoch 14/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1542 - acc: 0.9368 - val_loss: 0.1943 - val_acc: 0.9229\n",
      "Epoch 15/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1538 - acc: 0.9396 - val_loss: 0.1638 - val_acc: 0.9312\n",
      "Epoch 16/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1318 - acc: 0.9493 - val_loss: 0.1810 - val_acc: 0.9271\n",
      "Epoch 17/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1490 - acc: 0.9431 - val_loss: 0.1426 - val_acc: 0.9375\n",
      "Epoch 18/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1360 - acc: 0.9507 - val_loss: 0.1309 - val_acc: 0.9417\n",
      "Epoch 19/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1110 - acc: 0.9563 - val_loss: 0.1135 - val_acc: 0.9583\n",
      "Epoch 20/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0982 - acc: 0.9646 - val_loss: 0.1142 - val_acc: 0.9562\n",
      "Epoch 21/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1113 - acc: 0.9618 - val_loss: 0.1244 - val_acc: 0.9479\n",
      "Epoch 22/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1071 - acc: 0.9632 - val_loss: 0.1195 - val_acc: 0.9562\n",
      "Epoch 23/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0942 - acc: 0.9708 - val_loss: 0.1028 - val_acc: 0.9667\n",
      "Epoch 24/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0839 - acc: 0.9681 - val_loss: 0.0865 - val_acc: 0.9729\n",
      "Epoch 25/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0871 - acc: 0.9757 - val_loss: 0.1099 - val_acc: 0.9562\n",
      "Epoch 26/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0951 - acc: 0.9604 - val_loss: 0.0988 - val_acc: 0.9646\n",
      "Epoch 27/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0950 - acc: 0.9632 - val_loss: 0.1017 - val_acc: 0.9687\n",
      "Epoch 28/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0776 - acc: 0.9708 - val_loss: 0.0834 - val_acc: 0.9667\n",
      "Epoch 29/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0910 - acc: 0.9646 - val_loss: 0.1168 - val_acc: 0.9521\n",
      "Epoch 30/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0725 - acc: 0.9722 - val_loss: 0.0972 - val_acc: 0.9646\n",
      "Epoch 31/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0779 - acc: 0.9771 - val_loss: 0.1303 - val_acc: 0.9542\n",
      "Epoch 32/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0756 - acc: 0.9729 - val_loss: 0.1467 - val_acc: 0.9479\n",
      "Epoch 33/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0804 - acc: 0.9708 - val_loss: 0.0977 - val_acc: 0.9583\n",
      "Epoch 34/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0768 - acc: 0.9715 - val_loss: 0.1441 - val_acc: 0.9437\n",
      "Epoch 00034: early stopping\n",
      "4800/4800 [==============================] - 9s 2ms/step\n",
      "loss :  0.260379283577\n",
      "acc :  0.909375\n",
      "Train on 1440 samples, validate on 480 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 14s 10ms/step - loss: 0.7589 - acc: 0.6861 - val_loss: 0.5727 - val_acc: 0.7396\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.4740 - acc: 0.7896 - val_loss: 0.3985 - val_acc: 0.8333\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3994 - acc: 0.8333 - val_loss: 0.3660 - val_acc: 0.8583\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3626 - acc: 0.8708 - val_loss: 0.3200 - val_acc: 0.8562\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3145 - acc: 0.8771 - val_loss: 0.2634 - val_acc: 0.8854\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2664 - acc: 0.8889 - val_loss: 0.2464 - val_acc: 0.8958\n",
      "Epoch 7/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2526 - acc: 0.8931 - val_loss: 0.2685 - val_acc: 0.8833\n",
      "Epoch 8/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2284 - acc: 0.9014 - val_loss: 0.2229 - val_acc: 0.9146\n",
      "Epoch 9/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2342 - acc: 0.9104 - val_loss: 0.2294 - val_acc: 0.9146\n",
      "Epoch 10/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2261 - acc: 0.9097 - val_loss: 0.2481 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2063 - acc: 0.9236 - val_loss: 0.2261 - val_acc: 0.9187\n",
      "Epoch 12/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1876 - acc: 0.9319 - val_loss: 0.2168 - val_acc: 0.9208\n",
      "Epoch 13/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1588 - acc: 0.9382 - val_loss: 0.1784 - val_acc: 0.9313\n",
      "Epoch 14/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1603 - acc: 0.9403 - val_loss: 0.1660 - val_acc: 0.9437\n",
      "Epoch 15/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1779 - acc: 0.9319 - val_loss: 0.1654 - val_acc: 0.9354\n",
      "Epoch 16/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1809 - acc: 0.9292 - val_loss: 0.1826 - val_acc: 0.9292\n",
      "Epoch 17/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1647 - acc: 0.9340 - val_loss: 0.1500 - val_acc: 0.9313\n",
      "Epoch 18/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1392 - acc: 0.9472 - val_loss: 0.2203 - val_acc: 0.9104\n",
      "Epoch 19/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1251 - acc: 0.9528 - val_loss: 0.1552 - val_acc: 0.9354\n",
      "Epoch 20/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1325 - acc: 0.9458 - val_loss: 0.1564 - val_acc: 0.9375\n",
      "Epoch 21/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1215 - acc: 0.9514 - val_loss: 0.1111 - val_acc: 0.9542\n",
      "Epoch 22/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1091 - acc: 0.9597 - val_loss: 0.1605 - val_acc: 0.9375\n",
      "Epoch 23/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.1296 - acc: 0.9479 - val_loss: 0.1299 - val_acc: 0.9521\n",
      "Epoch 24/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.1115 - acc: 0.9618 - val_loss: 0.1071 - val_acc: 0.9521\n",
      "Epoch 25/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.0904 - acc: 0.9743 - val_loss: 0.0866 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1040 - acc: 0.9597 - val_loss: 0.1370 - val_acc: 0.9542\n",
      "Epoch 27/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0998 - acc: 0.9632 - val_loss: 0.1292 - val_acc: 0.9500\n",
      "Epoch 28/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1062 - acc: 0.9639 - val_loss: 0.1614 - val_acc: 0.9417\n",
      "Epoch 29/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1197 - acc: 0.9535 - val_loss: 0.1037 - val_acc: 0.9583\n",
      "Epoch 30/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0917 - acc: 0.9653 - val_loss: 0.0900 - val_acc: 0.9646\n",
      "Epoch 31/100\n",
      "1440/1440 [==============================] - 5s 4ms/step - loss: 0.0833 - acc: 0.9701 - val_loss: 0.0893 - val_acc: 0.9604\n",
      "Epoch 32/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0782 - acc: 0.9722 - val_loss: 0.0943 - val_acc: 0.9729\n",
      "Epoch 33/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0800 - acc: 0.9708 - val_loss: 0.0635 - val_acc: 0.9854\n",
      "Epoch 34/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0675 - acc: 0.9813 - val_loss: 0.0847 - val_acc: 0.9688\n",
      "Epoch 35/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0727 - acc: 0.9743 - val_loss: 0.0548 - val_acc: 0.9792\n",
      "Epoch 36/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0563 - acc: 0.9833 - val_loss: 0.0682 - val_acc: 0.9729\n",
      "Epoch 37/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0531 - acc: 0.9826 - val_loss: 0.0920 - val_acc: 0.9729\n",
      "Epoch 38/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0551 - acc: 0.9819 - val_loss: 0.0927 - val_acc: 0.9563\n",
      "Epoch 39/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0557 - acc: 0.9833 - val_loss: 0.0555 - val_acc: 0.9813\n",
      "Epoch 40/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0570 - acc: 0.9806 - val_loss: 0.0557 - val_acc: 0.9792\n",
      "Epoch 41/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0542 - acc: 0.9819 - val_loss: 0.0535 - val_acc: 0.9833\n",
      "Epoch 42/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0544 - acc: 0.9833 - val_loss: 0.0534 - val_acc: 0.9875\n",
      "Epoch 43/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0516 - acc: 0.9785 - val_loss: 0.0553 - val_acc: 0.9813\n",
      "Epoch 44/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0543 - acc: 0.9840 - val_loss: 0.0577 - val_acc: 0.9729\n",
      "Epoch 45/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0511 - acc: 0.9854 - val_loss: 0.0562 - val_acc: 0.9750\n",
      "Epoch 00045: early stopping\n",
      "4800/4800 [==============================] - 9s 2ms/step\n",
      "loss :  0.155255734355\n",
      "acc :  0.955833333333\n",
      "Train on 1440 samples, validate on 480 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 13s 9ms/step - loss: 0.7213 - acc: 0.7139 - val_loss: 0.5203 - val_acc: 0.7813\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.5332 - acc: 0.7743 - val_loss: 0.4588 - val_acc: 0.8167\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.4622 - acc: 0.8160 - val_loss: 0.3951 - val_acc: 0.8479\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3835 - acc: 0.8521 - val_loss: 0.3445 - val_acc: 0.8646\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3534 - acc: 0.8597 - val_loss: 0.2958 - val_acc: 0.8854\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.3019 - acc: 0.8778 - val_loss: 0.2538 - val_acc: 0.8938\n",
      "Epoch 7/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2577 - acc: 0.8951 - val_loss: 0.2463 - val_acc: 0.8938\n",
      "Epoch 8/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2305 - acc: 0.9153 - val_loss: 0.2012 - val_acc: 0.9208\n",
      "Epoch 9/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2275 - acc: 0.9063 - val_loss: 0.2336 - val_acc: 0.9062\n",
      "Epoch 10/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.2177 - acc: 0.9215 - val_loss: 0.2195 - val_acc: 0.9083\n",
      "Epoch 11/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1925 - acc: 0.9243 - val_loss: 0.2208 - val_acc: 0.9187\n",
      "Epoch 12/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1805 - acc: 0.9326 - val_loss: 0.2048 - val_acc: 0.9250\n",
      "Epoch 13/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1529 - acc: 0.9403 - val_loss: 0.1635 - val_acc: 0.9417\n",
      "Epoch 14/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1622 - acc: 0.9354 - val_loss: 0.1560 - val_acc: 0.9417\n",
      "Epoch 15/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1352 - acc: 0.9507 - val_loss: 0.1555 - val_acc: 0.9417\n",
      "Epoch 16/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1477 - acc: 0.9431 - val_loss: 0.1533 - val_acc: 0.9375\n",
      "Epoch 17/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1265 - acc: 0.9528 - val_loss: 0.1223 - val_acc: 0.9562\n",
      "Epoch 18/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1117 - acc: 0.9597 - val_loss: 0.1466 - val_acc: 0.9479\n",
      "Epoch 19/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1117 - acc: 0.9556 - val_loss: 0.1404 - val_acc: 0.9500\n",
      "Epoch 20/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1278 - acc: 0.9507 - val_loss: 0.1119 - val_acc: 0.9625\n",
      "Epoch 21/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1025 - acc: 0.9611 - val_loss: 0.1040 - val_acc: 0.9646\n",
      "Epoch 22/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1010 - acc: 0.9625 - val_loss: 0.1393 - val_acc: 0.9542\n",
      "Epoch 23/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1174 - acc: 0.9576 - val_loss: 0.1363 - val_acc: 0.9500\n",
      "Epoch 24/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1033 - acc: 0.9618 - val_loss: 0.1461 - val_acc: 0.9521\n",
      "Epoch 25/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.1270 - acc: 0.9479 - val_loss: 0.0962 - val_acc: 0.9625\n",
      "Epoch 26/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0986 - acc: 0.9639 - val_loss: 0.0963 - val_acc: 0.9667\n",
      "Epoch 27/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0869 - acc: 0.9688 - val_loss: 0.0953 - val_acc: 0.9708\n",
      "Epoch 28/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0810 - acc: 0.9674 - val_loss: 0.0768 - val_acc: 0.9771\n",
      "Epoch 29/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0821 - acc: 0.9701 - val_loss: 0.1160 - val_acc: 0.9563\n",
      "Epoch 30/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0693 - acc: 0.9771 - val_loss: 0.1117 - val_acc: 0.9563\n",
      "Epoch 31/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0758 - acc: 0.9715 - val_loss: 0.0819 - val_acc: 0.9708\n",
      "Epoch 32/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0724 - acc: 0.9708 - val_loss: 0.0677 - val_acc: 0.9729\n",
      "Epoch 33/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0970 - acc: 0.9653 - val_loss: 0.0921 - val_acc: 0.9583\n",
      "Epoch 34/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0842 - acc: 0.9694 - val_loss: 0.0927 - val_acc: 0.9646\n",
      "Epoch 35/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0670 - acc: 0.9792 - val_loss: 0.0657 - val_acc: 0.9792\n",
      "Epoch 36/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0558 - acc: 0.9833 - val_loss: 0.0661 - val_acc: 0.9708\n",
      "Epoch 37/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0590 - acc: 0.9813 - val_loss: 0.0563 - val_acc: 0.9813\n",
      "Epoch 38/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0572 - acc: 0.9826 - val_loss: 0.0529 - val_acc: 0.9792\n",
      "Epoch 39/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0439 - acc: 0.9847 - val_loss: 0.0375 - val_acc: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0492 - acc: 0.9861 - val_loss: 0.0412 - val_acc: 0.9875\n",
      "Epoch 41/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0372 - acc: 0.9910 - val_loss: 0.0481 - val_acc: 0.9854\n",
      "Epoch 42/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0549 - acc: 0.9826 - val_loss: 0.0550 - val_acc: 0.9792\n",
      "Epoch 43/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0363 - acc: 0.9896 - val_loss: 0.0585 - val_acc: 0.9813\n",
      "Epoch 44/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0336 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9938\n",
      "Epoch 45/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0313 - acc: 0.9938 - val_loss: 0.0303 - val_acc: 0.9938\n",
      "Epoch 46/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0273 - acc: 0.9903 - val_loss: 0.0300 - val_acc: 0.9917\n",
      "Epoch 47/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0296 - acc: 0.9917 - val_loss: 0.0279 - val_acc: 0.9958\n",
      "Epoch 48/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0216 - acc: 0.9972 - val_loss: 0.0295 - val_acc: 0.9917\n",
      "Epoch 49/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0324 - acc: 0.9889 - val_loss: 0.0454 - val_acc: 0.9875\n",
      "Epoch 50/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0267 - acc: 0.9938 - val_loss: 0.0383 - val_acc: 0.9896\n",
      "Epoch 51/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0355 - acc: 0.9889 - val_loss: 0.0433 - val_acc: 0.9875\n",
      "Epoch 52/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0245 - acc: 0.9958 - val_loss: 0.0379 - val_acc: 0.9917\n",
      "Epoch 53/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0248 - acc: 0.9917 - val_loss: 0.0404 - val_acc: 0.9896\n",
      "Epoch 54/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0280 - acc: 0.9924 - val_loss: 0.0323 - val_acc: 0.9896\n",
      "Epoch 55/100\n",
      "1440/1440 [==============================] - 6s 4ms/step - loss: 0.0235 - acc: 0.9958 - val_loss: 0.0365 - val_acc: 0.9938\n",
      "Epoch 00055: early stopping\n",
      "4800/4800 [==============================] - 9s 2ms/step\n",
      "loss :  0.150276602756\n",
      "acc :  0.965\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_epoch = 100\n",
    "isEarlyStopping = True\n",
    "n_patience = 10\n",
    "conv1_size_list = [(1, 1), (1, 7), (1,13)]\n",
    "conv2_size_list = [(n_channel, 25)]\n",
    "conv1_filter_list = [5, 10, 20]\n",
    "conv2_filter_list = [5, 10, 20]\n",
    "\n",
    "history_list4 = []\n",
    "score_list4 = []\n",
    "\n",
    "for i, conv1_size in enumerate(conv1_size_list):\n",
    "    history_list4.append([])\n",
    "    score_list4.append([])\n",
    "    for j, conv2_size in enumerate(conv2_size_list):   \n",
    "        # Setup model\n",
    "        model = ShallowConvNet(x_train.shape[1:], conv1_size, conv2_size, 5, 10)\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "        # Setup callbacks\n",
    "        history = AccLossHistory()\n",
    "\n",
    "        if isEarlyStopping:\n",
    "            earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=n_patience, verbose=1, mode='auto')\n",
    "            callbacks = [history, earlyStopping]\n",
    "        else:\n",
    "            callbacks = [history]\n",
    "\n",
    "        # Model fit\n",
    "        fit_hist = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=n_epoch,\n",
    "              verbose=1,\n",
    "              validation_split=0.25,\n",
    "              callbacks=callbacks,\n",
    "              shuffle=True)\n",
    "\n",
    "        # Record metrics\n",
    "        history_list4[i].append(history)\n",
    "        y_hat = model.predict_on_batch(x_test)\n",
    "        score = model.evaluate(x_test, y_test, verbose = 1)\n",
    "        score_list4[i].append(score)\n",
    "        \n",
    "        print(model.metrics_names[0], ': ', score[0])\n",
    "        print(model.metrics_names[1], ': ', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x4 = len(conv1_size_list)\n",
    "y4 = len(conv2_size_list)\n",
    "test_acc4 = np.empty((x4, y4))\n",
    "test_loss4 = np.empty((x4, y4))\n",
    "val_acc4 = np.empty((x4, y4))\n",
    "val_loss4 = np.empty((x4, y4))\n",
    "\n",
    "for i in range(x4):\n",
    "    for j in range(y4):\n",
    "        test_acc4[i][j] = score_list4[i][j][-1]\n",
    "        test_loss4[i][j] = score_list4[i][j][0]\n",
    "        val_acc4[i][j] = history_list4[i][j].acc[-1][-1]\n",
    "        val_loss4[i][j] = history_list4[i][j].loss[-1][0]\n",
    "        \n",
    "scores4 = np.array(score_list4)\n",
    "histories4 = np.array(history_list4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "docs_dir=os.path.expanduser('~/SSVEP_CNN/Results')\n",
    "np.save(os.path.join(docs_dir, 'ConvSize_3x1_test_acc'), test_acc4)\n",
    "np.save(os.path.join(docs_dir, 'ConvSize_3x1_val_acc'), val_acc4)\n",
    "np.save(os.path.join(docs_dir, 'ConvSize_3x1_val_loss'), val_loss4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Arguments:\n",
    "        data       : A 2D numpy array of shape (N,M)\n",
    "        row_labels : A list or array of length N with the labels\n",
    "                     for the rows\n",
    "        col_labels : A list or array of length M with the labels\n",
    "                     for the columns\n",
    "    Optional arguments:\n",
    "        ax         : A matplotlib.axes.Axes instance to which the heatmap\n",
    "                     is plotted. If not provided, use current axes or\n",
    "                     create a new one.\n",
    "        cbar_kw    : A dictionary with arguments to\n",
    "                     :meth:`matplotlib.Figure.colorbar`.\n",
    "        cbarlabel  : The label for the colorbar\n",
    "    All other arguments are directly passed on to the imshow call.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=False, bottom=True,\n",
    "                   labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Arguments:\n",
    "        im         : The AxesImage to be labeled.\n",
    "    Optional arguments:\n",
    "        data       : Data used to annotate. If None, the image's data is used.\n",
    "        valfmt     : The format of the annotations inside the heatmap.\n",
    "                     This should either use the string format method, e.g.\n",
    "                     \"$ {x:.2f}\", or be a :class:`matplotlib.ticker.Formatter`.\n",
    "        textcolors : A list or array of two color specifications. The first is\n",
    "                     used for values below a threshold, the second for those\n",
    "                     above.\n",
    "        threshold  : Value in data units according to which the colors from\n",
    "                     textcolors are applied. If None (the default) uses the\n",
    "                     middle of the colormap as separation.\n",
    "\n",
    "    Further arguments are passed on to the created text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[im.norm(data[i, j]) > threshold])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Conv2 Size')"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAFNCAYAAABrKOlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYuIiIjoYH8Bf0N4KZg2kZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bc5236470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''PLOTING MODULE'''\n",
    "fig = plt.figure(figsize=(13,5))\n",
    "\n",
    "val_plot = fig.add_subplot(121)\n",
    "# val_plot.imshow(val_acc, interpolation='nearest')\n",
    "# plt.legend(loc=4, borderaxespad=0.7)\n",
    "# plt.title('Validation Accuracy Graph')\n",
    "# plt.ylabel('Conv1 Size')\n",
    "# plt.xlabel('Conv2 Size')\n",
    "# plt.yticks(range(len(conv1_size_list)), conv1_size_list)\n",
    "# plt.xticks(range(len(conv2_size_list)), conv2_size_list)\n",
    "\n",
    "val_im, cbar = heatmap(val_acc4, conv1_size_list, conv2_size_list, ax=val_plot,\n",
    "                   cmap='magma_r', cbarlabel=\"Accuracy\")\n",
    "texts = annotate_heatmap(val_im, valfmt=\"{x:.3f}\")\n",
    "\n",
    "plt.title('Validation Accuracy Graph')\n",
    "plt.ylabel('Conv1 Size')\n",
    "plt.xlabel('Conv2 Size')\n",
    "\n",
    "test_plot = fig.add_subplot(122)\n",
    "# test_plot.imshow(test_acc, interpolation='nearest')\n",
    "# # plt.legend(loc=1, borderaxespad=0.7)\n",
    "# plt.title('Test Accuracy Graph')\n",
    "# plt.ylabel('Conv1 Size')\n",
    "# plt.xlabel('Conv2 Size')\n",
    "# plt.yticks(range(len(conv1_size_list)), conv1_size_list)\n",
    "# plt.xticks(range(len(conv2_size_list)), conv2_size_list)\n",
    "\n",
    "test_im, cbar = heatmap(test_acc4, conv1_size_list, conv2_size_list, ax=test_plot,\n",
    "                   cmap='magma_r', cbarlabel=\"Accuracy\")\n",
    "texts = annotate_heatmap(test_im, valfmt=\"{x:.3f}\")\n",
    "\n",
    "plt.title('Test Accuracy Graph')\n",
    "plt.ylabel('Conv1 Size')\n",
    "plt.xlabel('Conv2 Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo far, the best convolutional layer configuration is\\nConv1: No. of filters = 5, Filter Size = (1, 25)\\nConv2: No. of filters = 10, Filter Size = (9, 1)\\n'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So far, the best convolutional layer configuration is\n",
    "Conv1: No. of filters = 5, Filter Size = (1, 25)\n",
    "Conv2: No. of filters = 10, Filter Size = (9, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d4c0863edffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_size_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0macc_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Conv1 Size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_size_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborderaxespad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation Accuracy Graph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEzCAYAAACL0fx+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADtRJREFUeJzt3FGIpXd5x/HfY9JUqlZLs4WSTUxK\n19olFGKHYBGqRVuSXGxubElArBJcsI2FKkKKRUu8qlIKhbR2S8VWqGn0oi6ykgsbUcRIVmyDSQhs\nozVDhKwacyMa0z69mFOZjrOZM5szu489nw8MnPec/5x5+DOz3z3vnHmruwMAE73gYg8AAOciUgCM\nJVIAjCVSAIwlUgCMJVIAjLVnpKrqw1X1ZFV99RyPV1X9VVWdqaoHq+pVqx8TgHW0zCupjyS54Tke\nvzHJkcXH8SR/8/zHAoAlItXdn0vynedYcnOSf+wt9yd5WVX94qoGBGB9reJ3UlckeXzb8ebiPgB4\nXi5dwXPULvfteq2lqjqerVOCedGLXvTrr3zlK1fw5QGY7stf/vK3uvvQfj9vFZHaTHLltuPDSZ7Y\nbWF3n0hyIkk2Njb69OnTK/jyAExXVf95Pp+3itN9J5O8efEuv1cnebq7v7mC5wVgze35SqqqPpbk\ndUkur6rNJO9L8lNJ0t0fSnIqyU1JziT5XpK3HtSwAKyXPSPV3bfu8Xgn+cOVTQQAC644AcBYIgXA\nWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBY\nIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgi\nBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIF\nwFgiBcBYS0Wqqm6oqker6kxV3bHL41dV1X1V9ZWqerCqblr9qACsmz0jVVWXJLkryY1Jjia5taqO\n7lj2p0nu6e7rktyS5K9XPSgA62eZV1LXJznT3Y919zNJ7k5y8441neRnF7dfmuSJ1Y0IwLpaJlJX\nJHl82/Hm4r7t/izJm6pqM8mpJO/Y7Ymq6nhVna6q02fPnj2PcQFYJ8tEqna5r3cc35rkI919OMlN\nST5aVT/23N19ors3unvj0KFD+58WgLWyTKQ2k1y57fhwfvx03m1J7kmS7v5ikhcmuXwVAwKwvpaJ\n1ANJjlTVNVV1WbbeGHFyx5pvJHl9klTVr2YrUs7nAfC87Bmp7n42ye1J7k3ySLbexfdQVd1ZVccW\ny96V5G1V9e9JPpbkLd2985QgAOzLpcss6u5T2XpDxPb73rvt9sNJXrPa0QBYd644AcBYIgXAWCIF\nwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXA\nWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBY\nIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFgiBcBYIgXAWCIFwFhL\nRaqqbqiqR6vqTFXdcY41v1dVD1fVQ1X1T6sdE4B1dOleC6rqkiR3JfntJJtJHqiqk9398LY1R5L8\nSZLXdPdTVfULBzUwAOtjmVdS1yc5092PdfczSe5OcvOONW9Lcld3P5Uk3f3kascEYB0tE6krkjy+\n7Xhzcd92r0jyiqr6QlXdX1U3rGpAANbXnqf7ktQu9/Uuz3MkyeuSHE7y+aq6tru/+3+eqOp4kuNJ\nctVVV+17WADWyzKvpDaTXLnt+HCSJ3ZZ88nu/mF3fy3Jo9mK1v/R3Se6e6O7Nw4dOnS+MwOwJpaJ\n1ANJjlTVNVV1WZJbkpzcseZfkvxWklTV5dk6/ffYKgcFYP3sGanufjbJ7UnuTfJIknu6+6GqurOq\nji2W3Zvk21X1cJL7kry7u799UEMDsB6qe+evly6MjY2NPn369EX52gBcWFX15e7e2O/nueIEAGOJ\nFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kU\nAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQA\nY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABjiRQAY4kUAGOJFABj\niRQAYy0Vqaq6oaoeraozVXXHc6x7Y1V1VW2sbkQA1tWekaqqS5LcleTGJEeT3FpVR3dZ95Ikf5Tk\nS6seEoD1tMwrqeuTnOnux7r7mSR3J7l5l3XvT/KBJN9f4XwArLFlInVFkse3HW8u7vuRqrouyZXd\n/akVzgbAmlsmUrXLff2jB6tekOQvk7xrzyeqOl5Vp6vq9NmzZ5efEoC1tEykNpNcue34cJInth2/\nJMm1ST5bVV9P8uokJ3d780R3n+juje7eOHTo0PlPDcBaWCZSDyQ5UlXXVNVlSW5JcvJ/H+zup7v7\n8u6+uruvTnJ/kmPdffpAJgZgbewZqe5+NsntSe5N8kiSe7r7oaq6s6qOHfSAAKyvS5dZ1N2nkpza\ncd97z7H2dc9/LABwxQkABhMpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYS\nKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIp\nAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikA\nxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxloqUlV1Q1U9WlVnquqOXR5/Z1U9XFUPVtVnqurlqx8V\ngHWzZ6Sq6pIkdyW5McnRJLdW1dEdy76SZKO7fy3JJ5J8YNWDArB+lnkldX2SM939WHc/k+TuJDdv\nX9Dd93X39xaH9yc5vNoxAVhHy0TqiiSPbzveXNx3Lrcl+fRuD1TV8ao6XVWnz549u/yUAKylZSJV\nu9zXuy6selOSjSQf3O3x7j7R3RvdvXHo0KHlpwRgLV26xJrNJFduOz6c5Imdi6rqDUnek+S13f2D\n1YwHwDpb5pXUA0mOVNU1VXVZkluSnNy+oKquS/K3SY5195OrHxOAdbRnpLr72SS3J7k3ySNJ7unu\nh6rqzqo6tlj2wSQvTvLxqvq3qjp5jqcDgKUtc7ov3X0qyakd97132+03rHguAHDFCQDmEikAxhIp\nAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikA\nxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDG\nEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMYSKQDGEikAxhIpAMZa\nKlJVdUNVPVpVZ6rqjl0e/+mq+ufF41+qqqtXPSgA62fPSFXVJUnuSnJjkqNJbq2qozuW3Zbkqe7+\n5SR/meTPVz0oAOtnmVdS1yc5092PdfczSe5OcvOONTcn+YfF7U8keX1V1erGBGAdLROpK5I8vu14\nc3Hfrmu6+9kkTyf5+VUMCMD6unSJNbu9IurzWJOqOp7k+OLwB1X11SW+PlsuT/Ktiz3ETxD7tT/2\na3/s1/79yvl80jKR2kxy5bbjw0meOMeazaq6NMlLk3xn5xN194kkJ5Kkqk5398b5DL2O7Nf+2K/9\nsV/7Y7/2r6pOn8/nLXO674EkR6rqmqq6LMktSU7uWHMyye8vbr8xyb9294+9kgKA/djzlVR3P1tV\ntye5N8klST7c3Q9V1Z1JTnf3ySR/n+SjVXUmW6+gbjnIoQFYD8uc7kt3n0pyasd97912+/tJfnef\nX/vEPtevO/u1P/Zrf+zX/tiv/TuvPStn5QCYymWRABjrwCPlkkr7s8R+vbOqHq6qB6vqM1X18osx\n5xR77de2dW+sqq6qtX5H1jL7VVW/t/gee6iq/ulCzzjJEj+PV1XVfVX1lcXP5E0XY84pqurDVfXk\nuf68qLb81WI/H6yqV+35pN19YB/ZeqPFfyT5pSSXJfn3JEd3rPmDJB9a3L4lyT8f5EyTP5bcr99K\n8jOL22+3X8+9X4t1L0nyuST3J9m42HNP3q8kR5J8JcnPLY5/4WLPPXy/TiR5++L20SRfv9hzX+Q9\n+80kr0ry1XM8flOST2frb2tfneRLez3nQb+Sckml/dlzv7r7vu7+3uLw/mz93dq6Wub7K0nen+QD\nSb5/IYcbaJn9eluSu7r7qSTp7icv8IyTLLNfneRnF7dfmh//G9K10t2fyy5/I7vNzUn+sbfcn+Rl\nVfWLz/WcBx0pl1Tan2X2a7vbsvW/knW1535V1XVJruzuT13IwYZa5vvrFUleUVVfqKr7q+qGCzbd\nPMvs158leVNVbWbrHdDvuDCj/cTa779xy70F/XlY2SWV1sTSe1FVb0qykeS1BzrRbM+5X1X1gmxd\nlf8tF2qg4Zb5/ro0W6f8XpetV+mfr6pru/u7BzzbRMvs161JPtLdf1FVv5Gtvxe9trv/++DH+4m0\n73/vD/qV1H4uqZTnuqTSmlhmv1JVb0jyniTHuvsHF2i2ifbar5ckuTbJZ6vq69k6B35yjd88sezP\n4ye7+4fd/bUkj2YrWutomf26Lck9SdLdX0zywmxd14/dLfVv3HYHHSmXVNqfPfdrcfrqb7MVqHX+\nfUGyx35199PdfXl3X93dV2frd3jHuvu8riH2/8AyP4//kq0356SqLs/W6b/HLuiUcyyzX99I8vok\nqapfzVakzl7QKX+ynEzy5sW7/F6d5Onu/uZzfcKBnu5rl1TalyX364NJXpzk44v3l3yju49dtKEv\noiX3i4Ul9+veJL9TVQ8n+a8k7+7ub1+8qS+eJffrXUn+rqr+OFunrd6yxv/JTlV9LFunii9f/J7u\nfUl+Kkm6+0PZ+r3dTUnOJPlekrfu+ZxrvJ8ADOeKEwCMJVIAjCVSAIwlUgCMJVIAjCVSAIwlUgCM\nJVIAjPU/r6v9GT7o/nMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a62ebbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "acc_plot = fig.add_subplot(121)\n",
    "for i in range(len(conv1_size_list)):\n",
    "    his = history_list[i]\n",
    "    acc_plot.plot(np.array(his.acc)[:,1], label='Conv1 Size {}'.format(conv1_size_list[i]))\n",
    "plt.legend(loc=4, borderaxespad=0.7)\n",
    "plt.title('Validation Accuracy Graph')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Epochs')\n",
    "\n",
    "loss_plot = fig.add_subplot(122)\n",
    "for i in range(len(conv1_size_list)):\n",
    "    his = history_list[i]\n",
    "    loss_plot.plot(np.array(his.loss)[:,1], label='Conv1 Size {}'.format(conv1_size_list[i]))\n",
    "plt.legend(loc=1, borderaxespad=0.7)\n",
    "plt.title('Validation Loss Graph')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = np.array(score_list)\n",
    "t = range(len(conv1_size_list))\n",
    "\n",
    "fig, acc_axis = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "acc_axis.set_ylabel('Accuracy', color=color)\n",
    "acc_axis.plot(t, scores[:,1], 'x--', color=color)\n",
    "acc_axis.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "loss_axis = acc_axis.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "loss_axis.set_ylabel('Loss', color=color)  # we already handled the x-label with ax1\n",
    "loss_axis.plot(t, scores[:,0], 'o--', color=color)\n",
    "loss_axis.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Test Result')\n",
    "plt.xlabel('Conv1 Size')\n",
    "plt.xticks(t, conv1_size_list)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
